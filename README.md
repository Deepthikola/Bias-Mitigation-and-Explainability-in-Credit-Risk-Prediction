# Bias-Mitigation-and-Explainability-in-Credit-Risk-Prediction
Bias Mitigation and Explainability in Credit Risk Prediction Using Fairness aware Machine Learning Models and SHAP based Interpretability

In the financial services industry, the use of machine learning in credit scoring has grown as a way of rapidly assessing risk and dealing with large numbers of applications. The benefit is a potentially more efficient process, and the cost is less transparency. Many of today's scores are black boxes that are neither interpretable by the applicant, nor by the regulator, nor by the organizations' own analysts. With an opaque scoring process, consumers often do not fully understand, trust, or have a way to contest a decision that affects them in meaningful ways.
Machine learning algorithms are known to perpetuate and aggregate existing social and economic inequality. 

For instance, demographic, behavioural, and socio economic variables can emerge in financial datasets. These cases can result in discriminatory penalties for individuals with irregular banking patterns, unstable housing, or non-conventional employment records, who are denied loans and credit even when financially solvent. Furthermore, heuristics such as these are poor proxies for creditworthiness, yet inform many models with no ability to counter or contextualize these heuristics, which can effectively eliminate from participation groups such as freelancers/migrant workers, self-employed, or students with limited histories of savings.

One key criticism is that the rules that are used to remove bias are ineffective. The regulations mainly only target protected attributes like gender and age, and ignore characteristics relating to financial status or lifestyle that are proxy variables of socioeconomic status. This absence of guidance enables institutions to use ostensibly compliant models that encode discrimination through secondary traits, in addition to making it difficult for model accountability to progress due to institutions defending their use of models by referencing their technical complexity. 
Interpretable methods like SHAP are commonly used to explain credit decisions. While there is little inclusion of fairness at the subgroup level, the model's explanations amount to highly detailed accounts of how decisions are made, without indicating the presence of discriminatory models. Outside of restricted use, SHAP and other model agnostic explainers are misused, providing superficial transparency when greater scrutiny may be warranted. Most implementations also ignore temporal variation of these explanations or fairness patterns and may thus be particularly susceptible to fairness drift during economic shocks.

Currently, bias mitigation techniques deployed in practice focus on one shot treatment during training, with no systematic approach to continual fairness auditing, drift monitoring or re in forcing fairness constraints for ML lifecycle approaches that include a deployment phase. These feedback loops mean that the model continues to reinforce its decisions over time. For example, people denied credit may be labelled high risk, leading to further disadvantage of financially disadvantaged groups in future credit decisions. Though many studies tackle parts of the problem, none tackle it as a whole. Studies on "fairness through interpretability" often mistake interpretability with transparency, and studies on counterfactual or adversarial debiasing aren't tied to interpretability. None of these models can perform fairness auditing, multi level bias detection, subgroup based monitoring and mitigation, and explainability together as an integrated and actionable framework. Existing work is still entirely fragmented, and does not cover all the aspects of equity that arise during the application of credit modelling.

